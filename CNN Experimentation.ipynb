{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic CNN Framework for Landsat Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.plot import adjust_band\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import reshape_as_raster, reshape_as_image\n",
    "from rasterio.plot import show\n",
    "from rasterio.windows import Window\n",
    "from pyproj import Proj, transform\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dataset = rasterio.open('/deep_data/landcover_reproject.tif')\n",
    "label_image = label_dataset.read()\n",
    "\n",
    "image_paths = ['/deep_data/processed_landsat/LC08_CU_027012_20170907_20181121_C01_V01_SR_combined.tif',\n",
    "               '/deep_data/processed_landsat/LC08_CU_028012_20140814_20171017_C01_V01_SR_combined.tif',\n",
    "               '/deep_data/processed_landsat/LC08_CU_028011_20170907_20181130_C01_V01_SR_combined.tif',  \n",
    "               '/deep_data/processed_landsat/LC08_CU_028012_20171002_20171019_C01_V01_SR_combined.tif']\n",
    "\n",
    "landsat_datasets = []\n",
    "for fp in image_paths:\n",
    "    landsat_datasets.append(rasterio.open(fp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image projection:\n",
      "PROJCS[\"Albers\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378140,298.2569999999957,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]]]\n",
      "---\n",
      "Labels projection:\n",
      "EPSG:32618\n"
     ]
    }
   ],
   "source": [
    "# What is the raster's projection?\n",
    "image_proj = landsat_datasets[0].crs # 4326\n",
    "print('Image projection:')\n",
    "print(image_proj)\n",
    "print('---')\n",
    "# What is the raster's projection?\n",
    "label_proj = label_dataset.crs\n",
    "print('Labels projection:')\n",
    "print(label_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Dictionary and Confusion Matrix Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dict((\n",
    "(0,  'Background'),\n",
    "(1, 'Unclassified'),\n",
    "(2, 'High Intensity Developed'),\n",
    "(3, 'Medium Intensity Developed'),\n",
    "(4, 'Low Intensity Developed'),\n",
    "(5, 'Open Space Developed'),\n",
    "(6, 'Cultivated Land'),\n",
    "(7, 'Pasture/Hay'),\n",
    "(8, 'Grassland'),\n",
    "(9, 'Deciduous Forest'),\n",
    "(10, 'Evergreen Forest'),\n",
    "(11, 'Mixed Forest'),\n",
    "(12, 'Scrub/Shrub'),\n",
    "(13, 'Palustrine Forested Wetland'),\n",
    "(14, 'Palustrine Scrub/Shrub Wetland'),\n",
    "(15, 'Palustrine Emergent Wetland'),\n",
    "(16, 'Estuarine Forested Wetland'),\n",
    "(17, 'Estuarine Scrub/Shrub Wetland'),\n",
    "(18, 'Estuarine Emergent Wetland'),\n",
    "(19, 'Unconsolidated Shore'),\n",
    "(20, 'Bare Land'),\n",
    "(21, 'Water'),\n",
    "(22, 'Palustrine Aquatic Bed'),\n",
    "(23, 'Estuarine Aquatic Bed'),\n",
    "(24, 'Tundra'),\n",
    "(25, 'Snow/Ice')\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTO Run: \\n\\nnp.set_printoptions(precision=2)\\n\\n# Plot non-normalized confusion matrix\\nplot_confusion_matrix(np.argmax(sk_label_batch_val, axis=1), pred_index, classes=np.array(list(class_names)),\\n                      class_dict=class_names)\\n\\n# Plot normalized confusion matrix\\nplot_confusion_matrix(np.argmax(sk_label_batch_val, axis=1), pred_index, classes=np.array(list(class_names)),\\n                      class_dict=class_names,\\n                      normalize=True)\\n\\nplt.show()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, class_dict,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    # convert class_id to class_name using the class_dict\n",
    "    cover_names = []\n",
    "    for cover_class in classes:\n",
    "        cover_names.append(class_dict[cover_class])\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    else:\n",
    "        pass\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=cover_names, yticklabels=cover_names,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\"\"\"\n",
    "TO Run: \n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(np.argmax(sk_label_batch_val, axis=1), pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(np.argmax(sk_label_batch_val, axis=1), pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names,\n",
    "                      normalize=True)\n",
    "\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator and Prep Fcns\n",
    "\n",
    "This is a typical Keras generator that I've written to allow it to ingest a set of random pixel locations so we can randomly sample throughout the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_generator(image_datasets, label_dataset, tile_height, tile_width, pixel_locations, batch_size):\n",
    "    ### this is a keras compatible data generator which generates data and labels on the fly \n",
    "    ### from a set of pixel locations, a list of image datasets, and a label dataset\n",
    "    \n",
    "    # pixel locations looks like [r, c, dataset_index]\n",
    "    label_image = label_dataset.read()\n",
    "    label_image[label_image == 255] = 1\n",
    "\n",
    "    c = r = 0\n",
    "    i = 0\n",
    "    \n",
    "    outProj = Proj(label_dataset.crs)\n",
    "\n",
    "    # assuming all images have the same num of bands\n",
    "    band_count = image_datasets[0].count\n",
    "    class_count = len(np.unique(label_image))\n",
    "    buffer = math.ceil(tile_height / 2)\n",
    "  \n",
    "    while True:\n",
    "        image_batch = np.zeros((batch_size, tile_height, tile_width, 3)) # take one off because we don't want the QA band\n",
    "        label_batch = np.zeros((batch_size,class_count))\n",
    "        b = 0\n",
    "        while b < batch_size:\n",
    "            # if we're at the end  of the data just restart\n",
    "            if i >= len(pixel_locations):\n",
    "                i=0\n",
    "            c, r = pixel_locations[i][0]\n",
    "            dataset_index = pixel_locations[i][1]\n",
    "            i += 1\n",
    "            tile = image_datasets[dataset_index].read(list(np.arange(1, band_count+1)), window=Window(c-buffer, r-buffer, tile_width, tile_height))\n",
    "            if np.amax(tile) == 0: # don't include if it is part of the image with no pixels\n",
    "                pass\n",
    "            elif np.isnan(tile).any() == True or -9999 in tile: \n",
    "                # we don't want tiles containing nan or -999 this comes from edges\n",
    "                # this also takes a while and is inefficient\n",
    "                pass\n",
    "            elif tile.shape != (band_count, tile_width, tile_height):\n",
    "                print('wrong shape')\n",
    "                print(tile.shape)\n",
    "                # somehow we're randomly getting tiles without the correct dimensions\n",
    "                pass\n",
    "            elif np.isin(tile[7,:,:], [352, 368, 392, 416, 432, 480, 840, 864, 880, 904, 928, 944, 1352]).any() == True:\n",
    "                # make sure pixel doesn't contain clouds\n",
    "                # this is probably pretty inefficient but only checking width x height for each tile\n",
    "                # read more here: https://prd-wret.s3-us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/atoms/files/LSDS-1873_US_Landsat_ARD_DFCB_0.pdf\n",
    "                #print('Found some cloud.')\n",
    "                #print(tile[7,:,:])\n",
    "                pass\n",
    "            else:\n",
    "                tile = adjust_band(tile[0:7])\n",
    "                # reshape from raster format to image format\n",
    "                reshaped_tile = reshape_as_image(tile)\n",
    "                #print(reshaped_tile.shape)\n",
    "\n",
    "                # find gps of that pixel within the image\n",
    "                (x, y) = image_datasets[dataset_index].xy(r, c)\n",
    "\n",
    "                # convert the point we're sampling from to the same projection as the label dataset if necessary\n",
    "                inProj = Proj(image_datasets[dataset_index].crs)\n",
    "                if inProj != outProj:\n",
    "                    x,y = transform(inProj,outProj,x,y)\n",
    "\n",
    "                # reference gps in label_image\n",
    "                row, col = label_dataset.index(x,y)\n",
    "\n",
    "                # find label\n",
    "                label = label_image[:, row, col]\n",
    "                # if this label is part of the unclassified area then ignore\n",
    "                if label == 0 or np.isnan(label).any() == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    # add label to the batch in a one hot encoding style\n",
    "                    label_batch[b][label] = 1\n",
    "                    image_batch[b] = reshaped_tile[:,:,4:1:-1]\n",
    "                    b += 1\n",
    "        yield (image_batch, label_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a list of raster datasets and randomly samples `train_count` and `val_count` random pixels from each dataset.\n",
    "\n",
    "It doesn't sample within tile_size / 2 of the edge in order to avoid missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pixel_locations(image_datasets, train_count, val_count, tile_size):\n",
    "    ### this function pulls out a train_count + val_count number of random pixels from a list of raster datasets\n",
    "    ### and returns a list of training pixel locations and image indices \n",
    "    ### and a list of validation pixel locations and indices\n",
    "    \n",
    "    ## future improvements could make this select classes evenly\n",
    "    train_pixels = []\n",
    "    val_pixels = []\n",
    "    \n",
    "    buffer = math.ceil(tile_size/2)\n",
    "    \n",
    "    train_count_per_dataset = math.ceil(train_count / len(image_datasets))\n",
    "    val_count_per_dataset = math.ceil(val_count / len(image_datasets))\n",
    "   \n",
    "    total_count_per_dataset = train_count_per_dataset + val_count_per_dataset\n",
    "    for index, image_dataset in enumerate(tqdm(image_datasets)):\n",
    "        #randomly pick `count` num of pixels from each dataset\n",
    "        img_height, img_width = image_dataset.shape\n",
    "        \n",
    "        rows = range(0+buffer, img_height-buffer)\n",
    "        columns = range(0+buffer, img_width-buffer)\n",
    "        #rows_sub, columns_sub = zip(*random.sample(list(zip(rows, columns)), total_count))\n",
    "        \n",
    "        points = random.sample(set(itertools.product(rows, columns)), total_count_per_dataset)\n",
    "        \n",
    "        dataset_index_list = [index] * total_count_per_dataset\n",
    "        \n",
    "        dataset_pixels = list(zip(points, dataset_index_list))\n",
    "        \n",
    "        train_pixels += dataset_pixels[:train_count_per_dataset]\n",
    "        val_pixels += dataset_pixels[train_count_per_dataset:]\n",
    "        \n",
    "        \n",
    "    return (train_pixels, val_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out the generator and data prep functions\n",
    "\n",
    "Let's make sure all this data prep actually works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the training and validation pixel locations\n",
    "train_px, val_px = gen_pixel_locations(landsat_datasets, 100, 20, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image\n",
      "(2, 64, 64, 3)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n",
      "Image\n",
      "(2, 64, 64, 3)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n",
      "Image\n",
      "(2, 64, 64, 3)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n",
      "Image\n",
      "(2, 64, 64, 3)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# print out some image and label batches and check out their shapes\n",
    "im_batch = None\n",
    "\n",
    "count = 0\n",
    "for (im, label) in tile_generator(landsat_datasets, label_dataset, 64, 64, train_px, 2):\n",
    "    if count > 3:\n",
    "        break\n",
    "    print('Image')\n",
    "    print(im.shape)\n",
    "    print('Label')\n",
    "    print(label)\n",
    "    print(label.shape)\n",
    "    print('----')\n",
    "    count += 1\n",
    "    im_batch = im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually inspect an image patch\n",
    "\n",
    "While it shouldn't necessarily be recognizable it should look like it has data in it and that it varies somewhat from pixel to pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f376e073908>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(18, 5)) \n",
    "\n",
    "axs.imshow(im_batch[0,:,:,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get to the CNN Development!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "# The GPU id to use\n",
    "# Patrick \"0\"\n",
    "# Feroze  \"1\"\n",
    "# Yousuf  \"2\"\n",
    "# Diego   \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do other imports now...\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prep some of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "batch_size = 25\n",
    "label_image[label_image == 255] = 1\n",
    "num_classes = len(np.unique(label_image))\n",
    "epochs = 50\n",
    "\n",
    "# input image dimensions\n",
    "tile_side = 64\n",
    "img_rows, img_cols = tile_side, tile_side\n",
    "img_bands = landsat_datasets[0].count - 1\n",
    "\n",
    "weight_decay = 0.01\n",
    "\n",
    "input_shape = (img_rows, img_cols, img_bands)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the model\n",
    "\n",
    "This is just a simple CNN model but it should be able to perform well above random when predicting landcover types if everything is correct thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 62, 62, 64)        4096      \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 60, 60, 22)        12694     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 30, 30, 22)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 30, 30, 22)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 30, 30, 64)        12736     \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 23)                11799     \n",
      "=================================================================\n",
      "Total params: 2,659,501\n",
      "Trainable params: 2,659,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(tile_side, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(22, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 64, 64, 32)        2048      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 23)                188439    \n",
      "=================================================================\n",
      "Total params: 478,391\n",
      "Trainable params: 477,495\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "from keras.layers import Activation, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=input_shape))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    " \n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    " \n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the train/validation pixel locations to train with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_px, val_px = gen_pixel_locations(image_datasets=landsat_datasets, \n",
    "                                       train_count=5000, val_count=500, tile_size=tile_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up the remaining model hyperparameters and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#sgd = keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "metrics=['accuracy']\n",
    "\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN THE MODEL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[25,22,60,60] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d_22/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training_3/SGD/gradients/conv2d_22/convolution_grad/Conv2DBackpropFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_21/Relu, conv2d_22/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss_3/mul/_687}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_999_loss_3/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ec8afa82d432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_px\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtile_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlandsat_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile_side\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile_side\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_px\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_steps=len(val_px) // batch_size)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[25,22,60,60] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d_22/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training_3/SGD/gradients/conv2d_22/convolution_grad/Conv2DBackpropFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_21/Relu, conv2d_22/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss_3/mul/_687}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_999_loss_3/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator=tile_generator(landsat_datasets, label_dataset, tile_side, tile_side, train_px, batch_size), \n",
    "                    steps_per_epoch=len(train_px) // batch_size, epochs=epochs, verbose=1,\n",
    "                    validation_data=tile_generator(landsat_datasets, label_dataset, tile_side, tile_side, val_px, batch_size),\n",
    "                    validation_steps=len(val_px) // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's evaluate the Model\n",
    "\n",
    "We'll just generate 500 test pixels to evaluate it on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has a built in evaluate_generator function and because we told it above to use accuracy as a metric this function automatically outputs categorical accuracy which is what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(generator=tile_generator(landsat_datasets, label_dataset, 11, 11, val_px, batch_size), \n",
    "                        steps=len(val_px) // batch_size,\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this simple model we're getting 37% accuracy across 23 classes which is well above the random accuracy which would be around 4% (aka 1/23). That means we're in business!\n",
    "\n",
    "### Evaluating the model ourselves\n",
    "\n",
    "If we wanted to run this evaluation and take a look at specific predictions and labels we can do that below (albeit more inefficiently) just to get an intuitive understanding of what is going wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(generator=tile_generator([landsat_dataset], label_dataset, 11, 11, test_px, batch_size), \n",
    "                        steps=len(test_px) // batch_size,\n",
    "                         verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_generator = tile_generator([landsat_dataset], label_dataset, 11, 11, test_px, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.empty(predictions.shape)\n",
    "count = 0\n",
    "while count < len(test_px):\n",
    "    image_b, label_b = next(eval_generator)\n",
    "    labels[count] = label_b\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = np.argmax(labels, axis=1)     \n",
    "pred_index = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = np.zeros(pred_index.shape)\n",
    "correct_predictions[label_index == pred_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(correct_predictions) / len(test_px)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now maybe more informatively let's build a confusion matrix using the scikit-learn function.\n",
    "\n",
    "Read the docs here and make this more informative by following some of their examples: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names,\n",
    "                      normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn ML Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_generator(image_datasets, label_dataset, pixel_locations, batch_size):\n",
    "    ### this is a keras compatible data generator which generates data and labels on the fly \n",
    "    ### from a set of pixel locations, a list of image datasets, and a label dataset\n",
    "    \n",
    "    # pixel locations looks like [r, c, dataset_index]\n",
    "    label_image = label_dataset.read()\n",
    "    label_image[label_image == 255] = 1\n",
    "\n",
    "    c = r = 0\n",
    "    i = 0\n",
    "    \n",
    "    outProj = Proj(label_dataset.crs)\n",
    "\n",
    "    # assuming all images have the same num of bands\n",
    "    band_count = image_datasets[0].count\n",
    "    class_count = len(np.unique(label_image))\n",
    "  \n",
    "    image_batch = np.zeros((batch_size, band_count))\n",
    "    label_batch = np.zeros((batch_size, class_count))\n",
    "    b = 0\n",
    "    while b < batch_size:\n",
    "        # if we're at the end  of the data just restart\n",
    "        if i >= len(pixel_locations):\n",
    "            i=0\n",
    "        c, r = pixel_locations[i][0]\n",
    "        dataset_index = pixel_locations[i][1]\n",
    "        i += 1\n",
    "        tile = image_datasets[dataset_index].read(list(np.arange(1, band_count+1)), window=Window(c, r, 1, 1))\n",
    "        if np.amax(tile) == 0: # don't include if it is part of the image with no pixels\n",
    "            pass\n",
    "        elif np.isnan(tile).any() == True or -9999 in tile: \n",
    "            # we don't want tiles containing nan or -9999 this comes from edges\n",
    "            # this also takes a while and is inefficient\n",
    "            pass\n",
    "        else:\n",
    "            tile = adjust_band(tile)\n",
    "            # reshape from raster format to image format\n",
    "            reshaped_tile = reshape_as_image(tile)\n",
    "\n",
    "            # find gps of that pixel within the image\n",
    "            (x, y) = image_datasets[dataset_index].xy(r, c)\n",
    "\n",
    "            # convert the point we're sampling from to the same projection as the label dataset if necessary\n",
    "            inProj = Proj(image_datasets[dataset_index].crs)\n",
    "            if inProj != outProj:\n",
    "                x,y = transform(inProj,outProj,x,y)\n",
    "\n",
    "            # reference gps in label_image\n",
    "            row, col = label_dataset.index(x,y)\n",
    "\n",
    "            # find label\n",
    "            label = label_image[:, row, col]\n",
    "            # if this label is part of the unclassified area then ignore\n",
    "            if label == 0 or np.isnan(label).any() == True:\n",
    "                pass\n",
    "            else:\n",
    "                # add label to the batch in a one hot encoding style\n",
    "                label_batch[b][label] = 1\n",
    "                image_batch[b] = reshaped_tile\n",
    "                b += 1\n",
    "    return (image_batch, label_batch)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_tile_generator(image_datasets, label_dataset, tile_height, tile_width, pixel_locations, batch_size):\n",
    "    ### this is a keras compatible data generator which generates data and labels on the fly \n",
    "    ### from a set of pixel locations, a list of image datasets, and a label dataset\n",
    "    \n",
    "    # pixel locations looks like [r, c, dataset_index]\n",
    "    label_image = label_dataset.read()\n",
    "    label_image[label_image == 255] = 1\n",
    "\n",
    "    c = r = 0\n",
    "    i = 0\n",
    "    \n",
    "    outProj = Proj(label_dataset.crs)\n",
    "\n",
    "    # assuming all images have the same num of bands\n",
    "    band_count = image_datasets[0].count\n",
    "    class_count = len(np.unique(label_image))\n",
    "    buffer = math.ceil(tile_height / 2)\n",
    "  \n",
    "    while True:\n",
    "        image_batch = np.zeros((batch_size, tile_height * tile_width * band_count))\n",
    "        label_batch = np.zeros((batch_size,class_count))\n",
    "        b = 0\n",
    "        while b < batch_size:\n",
    "            # if we're at the end  of the data just restart\n",
    "            if i >= len(pixel_locations):\n",
    "                i=0\n",
    "            c, r = pixel_locations[i][0]\n",
    "            dataset_index = pixel_locations[i][1]\n",
    "            i += 1\n",
    "            tile = image_datasets[dataset_index].read(list(np.arange(1, band_count+1)), window=Window(c-buffer, r-buffer, tile_width, tile_height))\n",
    "            if np.amax(tile) == 0: # don't include if it is part of the image with no pixels\n",
    "                pass\n",
    "            elif np.isnan(tile).any() == True or -9999 in tile: \n",
    "                # we don't want tiles containing nan or -999 this comes from edges\n",
    "                # this also takes a while and is inefficient\n",
    "                pass\n",
    "            elif tile.shape != (band_count, tile_width, tile_height):\n",
    "                print('wrong shape')\n",
    "                # somehow we're randomly getting tiles without the correct dimensions\n",
    "                # I assume it is when the tiles are on the edge\n",
    "                pass\n",
    "            else:\n",
    "                tile = adjust_band(tile)\n",
    "                # reshape from raster format to image format\n",
    "                reshaped_tile = reshape_as_image(tile)\n",
    "\n",
    "                # find gps of that pixel within the image\n",
    "                (x, y) = image_datasets[dataset_index].xy(r, c)\n",
    "\n",
    "                # convert the point we're sampling from to the same projection as the label dataset if necessary\n",
    "                inProj = Proj(image_datasets[dataset_index].crs)\n",
    "                if inProj != outProj:\n",
    "                    x,y = transform(inProj,outProj,x,y)\n",
    "\n",
    "                # reference gps in label_image\n",
    "                row, col = label_dataset.index(x,y)\n",
    "\n",
    "                # find label\n",
    "                label = label_image[:, row, col]\n",
    "                # if this label is part of the unclassified area then ignore\n",
    "                if label == 0 or np.isnan(label).any() == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    # add label to the batch in a one hot encoding style\n",
    "                    label_batch[b][label] = 1\n",
    "                    image_batch[b] = reshaped_tile.flatten()\n",
    "                    b += 1\n",
    "        return (image_batch, label_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Data for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'landsat_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4c3239d6e8d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_px\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_px\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_pixel_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlandsat_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'landsat_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_px, val_px = gen_pixel_locations([landsat_dataset], 50000, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50000\n",
    "sk_im_batch, sk_label_batch = pixel_generator([landsat_dataset], label_dataset, train_px, batch_size)\n",
    "print(sk_im_batch.shape, sk_label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "sk_im_batch_val, sk_label_batch_val = pixel_generator([landsat_dataset], label_dataset, val_px, batch_size)\n",
    "print(sk_im_batch_val.shape, sk_label_batch_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "\n",
    "n_neighbors = 50\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
    "clf.fit(sk_im_batch, np.argmax(sk_label_batch, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = np.argmax(sk_label_batch_val, axis=1)\n",
    "\n",
    "clf.score(sk_im_batch_val, label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_index = clf.predict(sk_im_batch_val)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names,\n",
    "                      normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.argmax(sk_label_batch_val, axis=1), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred_index, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize our model with 500 trees\n",
    "rf = RandomForestClassifier(n_estimators=500, oob_score=True)\n",
    "\n",
    "# Fit our model to training data\n",
    "rf = rf.fit(sk_im_batch, np.argmax(sk_label_batch, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our OOB prediction of accuracy is: {oob}%'.format(oob=rf.oob_score_ * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(sk_im_batch_val, label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "for b, imp in zip(bands, rf.feature_importances_):\n",
    "    print('Band {b} importance: {imp}'.format(b=b, imp=imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_index = rf.predict(sk_im_batch_val)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names,\n",
    "                      normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred_index, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating training data that contains a 3x3 tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50000\n",
    "sk_im_batch, sk_label_batch = sk_tile_generator([landsat_dataset], label_dataset, 3, 3, train_px, batch_size)\n",
    "print(sk_im_batch.shape, sk_label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "sk_im_batch_val, sk_label_batch_val = sk_tile_generator([landsat_dataset], label_dataset, 3, 3, val_px, batch_size)\n",
    "print(sk_im_batch_val.shape, sk_label_batch_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500, oob_score=True)\n",
    "\n",
    "# Fit our model to training data\n",
    "rf = rf.fit(sk_im_batch, np.argmax(sk_label_batch, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our OOB prediction of accuracy is: {oob}%'.format(oob=rf.oob_score_ * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = np.argmax(sk_label_batch_val, axis=1)\n",
    "\n",
    "rf.score(sk_im_batch_val, label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "for b, imp in zip(bands, rf.feature_importances_):\n",
    "    print('Band {b} importance: {imp}'.format(b=b, imp=imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_index = rf.predict(sk_im_batch_val)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names,\n",
    "                      normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_clf = svm.SVC(gamma='scale')\n",
    "svm_clf.fit(sk_im_batch, np.argmax(sk_label_batch, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.score(sk_im_batch_val, np.argmax(sk_label_batch_val, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_index = svm_clf.predict(sk_im_batch_val)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names,\n",
    "                      normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a balanced percent of each class\n",
    "# look at the classes that are consistently mis classified\n",
    "# think about adding NDVI or taking out bands\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGNet16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "input_shape = (32,32,3)\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_px, val_px = gen_pixel_locations(image_datasets=landsat_datasets, \n",
    "                                       train_count=50000, val_count=5000, tile_size=tile_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 23)                23575     \n",
      "=================================================================\n",
      "Total params: 15,263,575\n",
      "Trainable params: 7,628,311\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vggmodel = keras.Sequential()\n",
    "vggmodel.add(vgg_conv)\n",
    "vggmodel.add(Flatten())\n",
    "vggmodel.add(Dense(1024, activation='relu'))\n",
    "vggmodel.add(Dropout(0.5))\n",
    "vggmodel.add(Dense(num_classes, activation='softmax'))\n",
    "vggmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "metrics=['accuracy']\n",
    "batch_size = 25\n",
    "\n",
    "vggmodel.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2000/2000 [==============================] - 78s 39ms/step - loss: 1.9195 - acc: 0.3645 - val_loss: 1.8984 - val_acc: 0.3578\n",
      "Epoch 2/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.8355 - acc: 0.3879 - val_loss: 1.8483 - val_acc: 0.3906\n",
      "Epoch 3/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.8117 - acc: 0.3917 - val_loss: 1.8651 - val_acc: 0.3718\n",
      "Epoch 4/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.7933 - acc: 0.3971 - val_loss: 1.8530 - val_acc: 0.3832\n",
      "Epoch 5/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.7844 - acc: 0.4010 - val_loss: 1.8998 - val_acc: 0.3944\n",
      "Epoch 6/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.7687 - acc: 0.4059 - val_loss: 1.8821 - val_acc: 0.3522\n",
      "Epoch 7/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 1.7722 - acc: 0.4066 - val_loss: 1.8443 - val_acc: 0.3996\n",
      "Epoch 8/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 1.7613 - acc: 0.4099 - val_loss: 1.8157 - val_acc: 0.3982\n",
      "Epoch 9/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 1.7332 - acc: 0.4150 - val_loss: 1.8252 - val_acc: 0.3958\n",
      "Epoch 10/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 1.7259 - acc: 0.4160 - val_loss: 1.8517 - val_acc: 0.3948\n",
      "Epoch 11/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 1.7201 - acc: 0.4159 - val_loss: 1.8556 - val_acc: 0.3912\n",
      "Epoch 12/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 1.7085 - acc: 0.4193 - val_loss: 1.8318 - val_acc: 0.4018\n",
      "Epoch 13/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 1.7039 - acc: 0.4227 - val_loss: 1.8500 - val_acc: 0.3930\n",
      "Epoch 14/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.6844 - acc: 0.4282 - val_loss: 1.8652 - val_acc: 0.3910\n",
      "Epoch 15/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 1.6709 - acc: 0.4329 - val_loss: 1.8417 - val_acc: 0.3970\n",
      "Epoch 16/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.6706 - acc: 0.4341 - val_loss: 1.9814 - val_acc: 0.3814\n",
      "Epoch 17/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.6595 - acc: 0.4344 - val_loss: 1.8372 - val_acc: 0.4162\n",
      "Epoch 18/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.6541 - acc: 0.4370 - val_loss: 1.9681 - val_acc: 0.3912\n",
      "Epoch 19/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 1.6464 - acc: 0.4426 - val_loss: 1.9218 - val_acc: 0.3862\n",
      "Epoch 20/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 1.6260 - acc: 0.4472 - val_loss: 1.9003 - val_acc: 0.3928\n",
      "Epoch 21/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 1.6164 - acc: 0.4490 - val_loss: 1.9598 - val_acc: 0.3646\n",
      "Epoch 22/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 1.5959 - acc: 0.4574 - val_loss: 1.9265 - val_acc: 0.3888\n",
      "Epoch 23/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 1.5967 - acc: 0.4588 - val_loss: 1.9346 - val_acc: 0.3848\n",
      "Epoch 24/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 1.6028 - acc: 0.4579 - val_loss: 1.9127 - val_acc: 0.4056\n",
      "Epoch 25/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 1.5993 - acc: 0.4563 - val_loss: 1.8973 - val_acc: 0.3952\n",
      "Epoch 26/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.5784 - acc: 0.4634 - val_loss: 1.9495 - val_acc: 0.3924\n",
      "Epoch 27/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 1.5837 - acc: 0.4626 - val_loss: 2.2799 - val_acc: 0.3550\n",
      "Epoch 28/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.5808 - acc: 0.4662 - val_loss: 2.0958 - val_acc: 0.3924\n",
      "Epoch 29/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 1.5686 - acc: 0.4705 - val_loss: 2.0129 - val_acc: 0.3728\n",
      "Epoch 30/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.5686 - acc: 0.4706 - val_loss: 2.0750 - val_acc: 0.3870\n",
      "Epoch 31/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 1.5666 - acc: 0.4706 - val_loss: 2.0000 - val_acc: 0.3810\n",
      "Epoch 32/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.5530 - acc: 0.4761 - val_loss: 2.0590 - val_acc: 0.3922\n",
      "Epoch 33/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 1.5478 - acc: 0.4775 - val_loss: 2.0987 - val_acc: 0.3940\n",
      "Epoch 34/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 1.5379 - acc: 0.4812 - val_loss: 2.0565 - val_acc: 0.3932\n",
      "Epoch 35/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 1.5727 - acc: 0.4766 - val_loss: 2.0545 - val_acc: 0.3806\n",
      "Epoch 36/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.5637 - acc: 0.4766 - val_loss: 2.1301 - val_acc: 0.3912\n",
      "Epoch 37/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.5865 - acc: 0.4709 - val_loss: 2.0775 - val_acc: 0.3842\n",
      "Epoch 38/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.5547 - acc: 0.4783 - val_loss: 2.1129 - val_acc: 0.3722\n",
      "Epoch 39/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.6011 - acc: 0.4668 - val_loss: 2.1560 - val_acc: 0.3918\n",
      "Epoch 40/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.6068 - acc: 0.4639 - val_loss: 2.2378 - val_acc: 0.3696\n",
      "Epoch 41/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 1.5660 - acc: 0.4763 - val_loss: 2.2118 - val_acc: 0.3812\n",
      "Epoch 42/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.5947 - acc: 0.4689 - val_loss: 2.1873 - val_acc: 0.3780\n",
      "Epoch 43/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.6016 - acc: 0.4712 - val_loss: 2.1031 - val_acc: 0.3844\n",
      "Epoch 44/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.5772 - acc: 0.4758 - val_loss: 2.1681 - val_acc: 0.3798\n",
      "Epoch 45/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.6055 - acc: 0.4688 - val_loss: 2.0345 - val_acc: 0.3796\n",
      "Epoch 46/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.6472 - acc: 0.4614 - val_loss: 2.0680 - val_acc: 0.3876\n",
      "Epoch 47/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.6308 - acc: 0.4622 - val_loss: 2.0196 - val_acc: 0.3870\n",
      "Epoch 48/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.6236 - acc: 0.4633 - val_loss: 2.1239 - val_acc: 0.3980\n",
      "Epoch 49/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 1.6119 - acc: 0.4668 - val_loss: 2.2913 - val_acc: 0.3704\n",
      "Epoch 50/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.6211 - acc: 0.4627 - val_loss: 2.2287 - val_acc: 0.3882\n",
      "Epoch 51/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 1.7262 - acc: 0.4409 - val_loss: 2.1031 - val_acc: 0.3640\n",
      "Epoch 52/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 1.8232 - acc: 0.4124 - val_loss: 2.0053 - val_acc: 0.3838\n",
      "Epoch 53/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 1.7768 - acc: 0.4242 - val_loss: 2.0542 - val_acc: 0.3692\n",
      "Epoch 54/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 1.7972 - acc: 0.4152 - val_loss: 2.0024 - val_acc: 0.3824\n",
      "Epoch 55/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 1.7716 - acc: 0.4248 - val_loss: 2.0293 - val_acc: 0.3848\n",
      "Epoch 56/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.8032 - acc: 0.4151 - val_loss: 1.9921 - val_acc: 0.3774\n",
      "Epoch 57/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 1.9462 - acc: 0.3711 - val_loss: 2.1785 - val_acc: 0.2622\n",
      "Epoch 58/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1431 - acc: 0.2731 - val_loss: 2.1769 - val_acc: 0.2574\n",
      "Epoch 59/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1793 - acc: 0.2560 - val_loss: 2.1810 - val_acc: 0.2530\n",
      "Epoch 60/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1779 - acc: 0.2558 - val_loss: 2.1742 - val_acc: 0.2576\n",
      "Epoch 61/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1754 - acc: 0.2563 - val_loss: 2.1801 - val_acc: 0.2554\n",
      "Epoch 62/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1744 - acc: 0.2555 - val_loss: 2.1837 - val_acc: 0.2554\n",
      "Epoch 63/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1721 - acc: 0.2560 - val_loss: 2.1928 - val_acc: 0.2558\n",
      "Epoch 64/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1692 - acc: 0.2583 - val_loss: 2.2249 - val_acc: 0.1830\n",
      "Epoch 65/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1640 - acc: 0.2587 - val_loss: 2.1824 - val_acc: 0.1848\n",
      "Epoch 66/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1637 - acc: 0.2596 - val_loss: 2.1833 - val_acc: 0.2560\n",
      "Epoch 67/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1778 - acc: 0.2553 - val_loss: 2.1772 - val_acc: 0.2558\n",
      "Epoch 68/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1779 - acc: 0.2553 - val_loss: 2.1646 - val_acc: 0.2616\n",
      "Epoch 69/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1758 - acc: 0.2570 - val_loss: 2.1662 - val_acc: 0.2602\n",
      "Epoch 70/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1761 - acc: 0.2558 - val_loss: 2.1702 - val_acc: 0.2576\n",
      "Epoch 71/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1777 - acc: 0.2559 - val_loss: 2.1779 - val_acc: 0.2536\n",
      "Epoch 72/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1755 - acc: 0.2560 - val_loss: 2.1735 - val_acc: 0.2570\n",
      "Epoch 73/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1740 - acc: 0.2551 - val_loss: 2.1791 - val_acc: 0.2554\n",
      "Epoch 74/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1723 - acc: 0.2558 - val_loss: 2.1857 - val_acc: 0.2562\n",
      "Epoch 75/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1710 - acc: 0.2565 - val_loss: 2.2224 - val_acc: 0.1840\n",
      "Epoch 76/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1647 - acc: 0.2595 - val_loss: 2.2094 - val_acc: 0.1824\n",
      "Epoch 77/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1602 - acc: 0.2593 - val_loss: 2.1756 - val_acc: 0.2564\n",
      "Epoch 78/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1737 - acc: 0.2565 - val_loss: 2.1796 - val_acc: 0.2556\n",
      "Epoch 79/300\n",
      "2000/2000 [==============================] - 71s 35ms/step - loss: 2.1765 - acc: 0.2554 - val_loss: 2.1768 - val_acc: 0.2554\n",
      "Epoch 80/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1769 - acc: 0.2563 - val_loss: 2.1625 - val_acc: 0.2616\n",
      "Epoch 81/300\n",
      "2000/2000 [==============================] - 71s 35ms/step - loss: 2.1759 - acc: 0.2562 - val_loss: 2.1653 - val_acc: 0.2602\n",
      "Epoch 82/300\n",
      "2000/2000 [==============================] - 71s 35ms/step - loss: 2.1771 - acc: 0.2557 - val_loss: 2.1679 - val_acc: 0.2578\n",
      "Epoch 83/300\n",
      "2000/2000 [==============================] - 71s 35ms/step - loss: 2.1764 - acc: 0.2558 - val_loss: 2.1783 - val_acc: 0.2536\n",
      "Epoch 84/300\n",
      "2000/2000 [==============================] - 71s 35ms/step - loss: 2.1737 - acc: 0.2561 - val_loss: 2.1741 - val_acc: 0.2570\n",
      "Epoch 85/300\n",
      "2000/2000 [==============================] - 71s 35ms/step - loss: 2.1737 - acc: 0.2552 - val_loss: 2.1792 - val_acc: 0.2560\n",
      "Epoch 86/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1712 - acc: 0.2560 - val_loss: 2.1908 - val_acc: 0.2554\n",
      "Epoch 87/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1687 - acc: 0.2583 - val_loss: 2.2251 - val_acc: 0.1836\n",
      "Epoch 88/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1634 - acc: 0.2587 - val_loss: 2.1868 - val_acc: 0.1832\n",
      "Epoch 89/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1626 - acc: 0.2595 - val_loss: 2.1813 - val_acc: 0.2560\n",
      "Epoch 90/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1773 - acc: 0.2554 - val_loss: 2.1781 - val_acc: 0.2556\n",
      "Epoch 91/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1778 - acc: 0.2553 - val_loss: 2.1765 - val_acc: 0.2550\n",
      "Epoch 92/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1755 - acc: 0.2569 - val_loss: 2.1621 - val_acc: 0.2614\n",
      "Epoch 93/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1756 - acc: 0.2559 - val_loss: 2.1613 - val_acc: 0.2614\n",
      "Epoch 94/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1775 - acc: 0.2560 - val_loss: 2.1698 - val_acc: 0.2574\n",
      "Epoch 95/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1757 - acc: 0.2559 - val_loss: 2.1772 - val_acc: 0.2538\n",
      "Epoch 96/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1735 - acc: 0.2554 - val_loss: 2.1748 - val_acc: 0.2568\n",
      "Epoch 97/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1724 - acc: 0.2557 - val_loss: 2.1827 - val_acc: 0.2562\n",
      "Epoch 98/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1710 - acc: 0.2563 - val_loss: 2.2174 - val_acc: 0.1834\n",
      "Epoch 99/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1651 - acc: 0.2595 - val_loss: 2.2100 - val_acc: 0.1832\n",
      "Epoch 100/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1602 - acc: 0.2594 - val_loss: 2.1809 - val_acc: 0.2532\n",
      "Epoch 101/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1732 - acc: 0.2566 - val_loss: 2.1776 - val_acc: 0.2566\n",
      "Epoch 102/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1765 - acc: 0.2555 - val_loss: 2.1771 - val_acc: 0.2550\n",
      "Epoch 103/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1770 - acc: 0.2562 - val_loss: 2.1770 - val_acc: 0.2548\n",
      "Epoch 104/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1760 - acc: 0.2562 - val_loss: 2.1616 - val_acc: 0.2614\n",
      "Epoch 105/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1770 - acc: 0.2556 - val_loss: 2.1595 - val_acc: 0.2614\n",
      "Epoch 106/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1764 - acc: 0.2558 - val_loss: 2.1697 - val_acc: 0.2578\n",
      "Epoch 107/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1739 - acc: 0.2561 - val_loss: 2.1778 - val_acc: 0.2542\n",
      "Epoch 108/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1739 - acc: 0.2551 - val_loss: 2.1766 - val_acc: 0.2556\n",
      "Epoch 109/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1712 - acc: 0.2559 - val_loss: 2.1874 - val_acc: 0.2568\n",
      "Epoch 110/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1689 - acc: 0.2582 - val_loss: 2.2227 - val_acc: 0.1838\n",
      "Epoch 111/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1637 - acc: 0.2586 - val_loss: 2.1908 - val_acc: 0.1830\n",
      "Epoch 112/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1625 - acc: 0.2594 - val_loss: 2.1845 - val_acc: 0.2546\n",
      "Epoch 113/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1772 - acc: 0.2555 - val_loss: 2.1772 - val_acc: 0.2552\n",
      "Epoch 114/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1780 - acc: 0.2551 - val_loss: 2.1765 - val_acc: 0.2552\n",
      "Epoch 115/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1758 - acc: 0.2570 - val_loss: 2.1748 - val_acc: 0.2552\n",
      "Epoch 116/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1759 - acc: 0.2556 - val_loss: 2.1627 - val_acc: 0.2610\n",
      "Epoch 117/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1778 - acc: 0.2559 - val_loss: 2.1589 - val_acc: 0.2612\n",
      "Epoch 118/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1757 - acc: 0.2560 - val_loss: 2.1704 - val_acc: 0.2584\n",
      "Epoch 119/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1740 - acc: 0.2553 - val_loss: 2.1778 - val_acc: 0.2548\n",
      "Epoch 120/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1724 - acc: 0.2559 - val_loss: 2.1798 - val_acc: 0.2548\n",
      "Epoch 121/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1715 - acc: 0.2560 - val_loss: 2.2110 - val_acc: 0.1832\n",
      "Epoch 122/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1654 - acc: 0.2595 - val_loss: 2.2092 - val_acc: 0.1836\n",
      "Epoch 123/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1603 - acc: 0.2596 - val_loss: 2.1833 - val_acc: 0.2552\n",
      "Epoch 124/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1730 - acc: 0.2569 - val_loss: 2.1809 - val_acc: 0.2552\n",
      "Epoch 125/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1768 - acc: 0.2553 - val_loss: 2.1771 - val_acc: 0.2544\n",
      "Epoch 126/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1770 - acc: 0.2563 - val_loss: 2.1745 - val_acc: 0.2558\n",
      "Epoch 127/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1762 - acc: 0.2565 - val_loss: 2.1766 - val_acc: 0.2554\n",
      "Epoch 128/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1772 - acc: 0.2555 - val_loss: 2.1623 - val_acc: 0.2604\n",
      "Epoch 129/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1768 - acc: 0.2559 - val_loss: 2.1585 - val_acc: 0.2610\n",
      "Epoch 130/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1743 - acc: 0.2561 - val_loss: 2.1710 - val_acc: 0.2590\n",
      "Epoch 131/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1741 - acc: 0.2550 - val_loss: 2.1789 - val_acc: 0.2544\n",
      "Epoch 132/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1715 - acc: 0.2559 - val_loss: 2.1837 - val_acc: 0.2546\n",
      "Epoch 133/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1698 - acc: 0.2581 - val_loss: 2.2236 - val_acc: 0.1822\n",
      "Epoch 134/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1633 - acc: 0.2586 - val_loss: 2.1878 - val_acc: 0.1838\n",
      "Epoch 135/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1623 - acc: 0.2597 - val_loss: 2.1873 - val_acc: 0.2556\n",
      "Epoch 136/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1778 - acc: 0.2551 - val_loss: 2.1811 - val_acc: 0.2552\n",
      "Epoch 137/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1781 - acc: 0.2551 - val_loss: 2.1760 - val_acc: 0.2544\n",
      "Epoch 138/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1760 - acc: 0.2568 - val_loss: 2.1724 - val_acc: 0.2554\n",
      "Epoch 139/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1763 - acc: 0.2556 - val_loss: 2.1758 - val_acc: 0.2558\n",
      "Epoch 140/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1779 - acc: 0.2558 - val_loss: 2.1635 - val_acc: 0.2600\n",
      "Epoch 141/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1760 - acc: 0.2558 - val_loss: 2.1622 - val_acc: 0.2610\n",
      "Epoch 142/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1741 - acc: 0.2552 - val_loss: 2.1695 - val_acc: 0.2588\n",
      "Epoch 143/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1726 - acc: 0.2557 - val_loss: 2.1821 - val_acc: 0.2550\n",
      "Epoch 144/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1718 - acc: 0.2558 - val_loss: 2.2045 - val_acc: 0.1842\n",
      "Epoch 145/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1662 - acc: 0.2592 - val_loss: 2.2065 - val_acc: 0.1820\n",
      "Epoch 146/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1603 - acc: 0.2594 - val_loss: 2.1813 - val_acc: 0.2548\n",
      "Epoch 147/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1728 - acc: 0.2567 - val_loss: 2.1846 - val_acc: 0.2558\n",
      "Epoch 148/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1773 - acc: 0.2549 - val_loss: 2.1797 - val_acc: 0.2546\n",
      "Epoch 149/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1770 - acc: 0.2559 - val_loss: 2.1748 - val_acc: 0.2552\n",
      "Epoch 150/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1768 - acc: 0.2560 - val_loss: 2.1720 - val_acc: 0.2552\n",
      "Epoch 151/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1772 - acc: 0.2551 - val_loss: 2.1750 - val_acc: 0.2558\n",
      "Epoch 152/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1771 - acc: 0.2555 - val_loss: 2.1654 - val_acc: 0.2592\n",
      "Epoch 153/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1745 - acc: 0.2558 - val_loss: 2.1647 - val_acc: 0.2612\n",
      "Epoch 154/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1745 - acc: 0.2546 - val_loss: 2.1702 - val_acc: 0.2594\n",
      "Epoch 155/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1720 - acc: 0.2556 - val_loss: 2.1852 - val_acc: 0.2552\n",
      "Epoch 156/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1699 - acc: 0.2576 - val_loss: 2.2176 - val_acc: 0.1844\n",
      "Epoch 157/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1633 - acc: 0.2584 - val_loss: 2.1882 - val_acc: 0.1822\n",
      "Epoch 158/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1622 - acc: 0.2593 - val_loss: 2.1857 - val_acc: 0.2546\n",
      "Epoch 159/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1781 - acc: 0.2549 - val_loss: 2.1834 - val_acc: 0.2558\n",
      "Epoch 160/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1784 - acc: 0.2546 - val_loss: 2.1790 - val_acc: 0.2554\n",
      "Epoch 161/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1762 - acc: 0.2564 - val_loss: 2.1745 - val_acc: 0.2540\n",
      "Epoch 162/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1766 - acc: 0.2553 - val_loss: 2.1703 - val_acc: 0.2558\n",
      "Epoch 163/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1781 - acc: 0.2553 - val_loss: 2.1743 - val_acc: 0.2558\n",
      "Epoch 164/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1763 - acc: 0.2554 - val_loss: 2.1680 - val_acc: 0.2590\n",
      "Epoch 165/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1748 - acc: 0.2547 - val_loss: 2.1654 - val_acc: 0.2612\n",
      "Epoch 166/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1728 - acc: 0.2553 - val_loss: 2.1732 - val_acc: 0.2592\n",
      "Epoch 167/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1719 - acc: 0.2553 - val_loss: 2.2035 - val_acc: 0.1832\n",
      "Epoch 168/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1663 - acc: 0.2588 - val_loss: 2.2025 - val_acc: 0.1850\n",
      "Epoch 169/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1609 - acc: 0.2591 - val_loss: 2.1774 - val_acc: 0.2578\n",
      "Epoch 170/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1725 - acc: 0.2567 - val_loss: 2.1837 - val_acc: 0.2544\n",
      "Epoch 171/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1779 - acc: 0.2544 - val_loss: 2.1814 - val_acc: 0.2562\n",
      "Epoch 172/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1768 - acc: 0.2558 - val_loss: 2.1787 - val_acc: 0.2556\n",
      "Epoch 173/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1770 - acc: 0.2556 - val_loss: 2.1761 - val_acc: 0.2534\n",
      "Epoch 174/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1777 - acc: 0.2548 - val_loss: 2.1682 - val_acc: 0.2566\n",
      "Epoch 175/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1772 - acc: 0.2552 - val_loss: 2.1746 - val_acc: 0.2550\n",
      "Epoch 176/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1748 - acc: 0.2554 - val_loss: 2.1705 - val_acc: 0.2588\n",
      "Epoch 177/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1749 - acc: 0.2543 - val_loss: 2.1666 - val_acc: 0.2614\n",
      "Epoch 178/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1724 - acc: 0.2552 - val_loss: 2.1796 - val_acc: 0.2590\n",
      "Epoch 179/300\n",
      "2000/2000 [==============================] - 71s 35ms/step - loss: 2.1701 - acc: 0.2574 - val_loss: 2.2168 - val_acc: 0.1834\n",
      "Epoch 180/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1636 - acc: 0.2580 - val_loss: 2.1880 - val_acc: 0.1850\n",
      "Epoch 181/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1620 - acc: 0.2592 - val_loss: 2.1817 - val_acc: 0.2578\n",
      "Epoch 182/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1786 - acc: 0.2544 - val_loss: 2.1801 - val_acc: 0.2550\n",
      "Epoch 183/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1789 - acc: 0.2543 - val_loss: 2.1814 - val_acc: 0.2564\n",
      "Epoch 184/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1762 - acc: 0.2560 - val_loss: 2.1777 - val_acc: 0.2552\n",
      "Epoch 185/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1769 - acc: 0.2549 - val_loss: 2.1748 - val_acc: 0.2532\n",
      "Epoch 186/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1782 - acc: 0.2550 - val_loss: 2.1681 - val_acc: 0.2570\n",
      "Epoch 187/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1767 - acc: 0.2549 - val_loss: 2.1765 - val_acc: 0.2546\n",
      "Epoch 188/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1748 - acc: 0.2545 - val_loss: 2.1708 - val_acc: 0.2584\n",
      "Epoch 189/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1734 - acc: 0.2549 - val_loss: 2.1708 - val_acc: 0.2614\n",
      "Epoch 190/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1720 - acc: 0.2549 - val_loss: 2.1962 - val_acc: 0.1828\n",
      "Epoch 191/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1666 - acc: 0.2585 - val_loss: 2.1998 - val_acc: 0.1836\n",
      "Epoch 192/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1612 - acc: 0.2587 - val_loss: 2.1805 - val_acc: 0.2528\n",
      "Epoch 193/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1723 - acc: 0.2564 - val_loss: 2.1775 - val_acc: 0.2584\n",
      "Epoch 194/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1779 - acc: 0.2542 - val_loss: 2.1796 - val_acc: 0.2550\n",
      "Epoch 195/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1774 - acc: 0.2551 - val_loss: 2.1809 - val_acc: 0.2558\n",
      "Epoch 196/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1774 - acc: 0.2553 - val_loss: 2.1772 - val_acc: 0.2556\n",
      "Epoch 197/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1776 - acc: 0.2545 - val_loss: 2.1751 - val_acc: 0.2528\n",
      "Epoch 198/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1775 - acc: 0.2548 - val_loss: 2.1672 - val_acc: 0.2574\n",
      "Epoch 199/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1751 - acc: 0.2551 - val_loss: 2.1776 - val_acc: 0.2546\n",
      "Epoch 200/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1752 - acc: 0.2541 - val_loss: 2.1733 - val_acc: 0.2574\n",
      "Epoch 201/300\n",
      "2000/2000 [==============================] - 71s 35ms/step - loss: 2.1726 - acc: 0.2549 - val_loss: 2.1762 - val_acc: 0.2616\n",
      "Epoch 202/300\n",
      "2000/2000 [==============================] - 71s 35ms/step - loss: 2.1706 - acc: 0.2571 - val_loss: 2.2147 - val_acc: 0.1830\n",
      "Epoch 203/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1637 - acc: 0.2577 - val_loss: 2.1825 - val_acc: 0.1832\n",
      "Epoch 204/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1619 - acc: 0.2591 - val_loss: 2.1860 - val_acc: 0.2532\n",
      "Epoch 205/300\n",
      "2000/2000 [==============================] - 71s 36ms/step - loss: 2.1789 - acc: 0.2543 - val_loss: 2.1766 - val_acc: 0.2578\n",
      "Epoch 206/300\n",
      "2000/2000 [==============================] - 71s 35ms/step - loss: 2.1790 - acc: 0.2539 - val_loss: 2.1784 - val_acc: 0.2552\n",
      "Epoch 207/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1764 - acc: 0.2558 - val_loss: 2.1811 - val_acc: 0.2558\n",
      "Epoch 208/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1771 - acc: 0.2548 - val_loss: 2.1755 - val_acc: 0.2554\n",
      "Epoch 209/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1785 - acc: 0.2547 - val_loss: 2.1746 - val_acc: 0.2534\n",
      "Epoch 210/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1771 - acc: 0.2546 - val_loss: 2.1698 - val_acc: 0.2568\n",
      "Epoch 211/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1749 - acc: 0.2545 - val_loss: 2.1762 - val_acc: 0.2554\n",
      "Epoch 212/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1738 - acc: 0.2546 - val_loss: 2.1768 - val_acc: 0.2562\n",
      "Epoch 213/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1724 - acc: 0.2548 - val_loss: 2.1935 - val_acc: 0.1852\n",
      "Epoch 214/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1668 - acc: 0.2585 - val_loss: 2.1960 - val_acc: 0.1820\n",
      "Epoch 215/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1620 - acc: 0.2585 - val_loss: 2.1714 - val_acc: 0.2574\n",
      "Epoch 216/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1716 - acc: 0.2565 - val_loss: 2.1833 - val_acc: 0.2532\n",
      "Epoch 217/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1781 - acc: 0.2540 - val_loss: 2.1767 - val_acc: 0.2574\n",
      "Epoch 218/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1774 - acc: 0.2551 - val_loss: 2.1772 - val_acc: 0.2554\n",
      "Epoch 219/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1779 - acc: 0.2552 - val_loss: 2.1796 - val_acc: 0.2556\n",
      "Epoch 220/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1780 - acc: 0.2544 - val_loss: 2.1763 - val_acc: 0.2552\n",
      "Epoch 221/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1776 - acc: 0.2548 - val_loss: 2.1734 - val_acc: 0.2542\n",
      "Epoch 222/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1755 - acc: 0.2551 - val_loss: 2.1725 - val_acc: 0.2560\n",
      "Epoch 223/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1753 - acc: 0.2541 - val_loss: 2.1769 - val_acc: 0.2562\n",
      "Epoch 224/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1728 - acc: 0.2549 - val_loss: 2.1800 - val_acc: 0.2560\n",
      "Epoch 225/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1709 - acc: 0.2569 - val_loss: 2.2143 - val_acc: 0.1852\n",
      "Epoch 226/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1640 - acc: 0.2577 - val_loss: 2.1793 - val_acc: 0.1822\n",
      "Epoch 227/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1618 - acc: 0.2592 - val_loss: 2.1762 - val_acc: 0.2574\n",
      "Epoch 228/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1790 - acc: 0.2544 - val_loss: 2.1823 - val_acc: 0.2534\n",
      "Epoch 229/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1790 - acc: 0.2539 - val_loss: 2.1754 - val_acc: 0.2574\n",
      "Epoch 230/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1772 - acc: 0.2556 - val_loss: 2.1766 - val_acc: 0.2552\n",
      "Epoch 231/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1768 - acc: 0.2551 - val_loss: 2.1782 - val_acc: 0.2558\n",
      "Epoch 232/300\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 2.1786 - acc: 0.2547 - val_loss: 2.1768 - val_acc: 0.2554\n",
      "Epoch 233/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1776 - acc: 0.2545 - val_loss: 2.1752 - val_acc: 0.2544\n",
      "Epoch 234/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1753 - acc: 0.2544 - val_loss: 2.1721 - val_acc: 0.2556\n",
      "Epoch 235/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1738 - acc: 0.2547 - val_loss: 2.1786 - val_acc: 0.2560\n",
      "Epoch 236/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1729 - acc: 0.2546 - val_loss: 2.1939 - val_acc: 0.1832\n",
      "Epoch 237/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1670 - acc: 0.2585 - val_loss: 2.1944 - val_acc: 0.1852\n",
      "Epoch 238/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1623 - acc: 0.2584 - val_loss: 2.1666 - val_acc: 0.2602\n",
      "Epoch 239/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1715 - acc: 0.2566 - val_loss: 2.1725 - val_acc: 0.2576\n",
      "Epoch 240/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1782 - acc: 0.2541 - val_loss: 2.1828 - val_acc: 0.2538\n",
      "Epoch 241/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1779 - acc: 0.2547 - val_loss: 2.1733 - val_acc: 0.2566\n",
      "Epoch 242/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1778 - acc: 0.2553 - val_loss: 2.1759 - val_acc: 0.2558\n",
      "Epoch 243/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1782 - acc: 0.2543 - val_loss: 2.1761 - val_acc: 0.2562\n",
      "Epoch 244/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1779 - acc: 0.2546 - val_loss: 2.1764 - val_acc: 0.2552\n",
      "Epoch 245/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1759 - acc: 0.2549 - val_loss: 2.1780 - val_acc: 0.2536\n",
      "Epoch 246/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1754 - acc: 0.2538 - val_loss: 2.1738 - val_acc: 0.2566\n",
      "Epoch 247/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1733 - acc: 0.2547 - val_loss: 2.1809 - val_acc: 0.2556\n",
      "Epoch 248/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1711 - acc: 0.2566 - val_loss: 2.2154 - val_acc: 0.1828\n",
      "Epoch 249/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1643 - acc: 0.2574 - val_loss: 2.1782 - val_acc: 0.1852\n",
      "Epoch 250/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1615 - acc: 0.2590 - val_loss: 2.1665 - val_acc: 0.2604\n",
      "Epoch 251/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1795 - acc: 0.2539 - val_loss: 2.1725 - val_acc: 0.2580\n",
      "Epoch 252/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1789 - acc: 0.2536 - val_loss: 2.1807 - val_acc: 0.2536\n",
      "Epoch 253/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1777 - acc: 0.2552 - val_loss: 2.1718 - val_acc: 0.2570\n",
      "Epoch 254/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1772 - acc: 0.2546 - val_loss: 2.1748 - val_acc: 0.2562\n",
      "Epoch 255/300\n",
      "2000/2000 [==============================] - 71s 35ms/step - loss: 2.1785 - acc: 0.2543 - val_loss: 2.1767 - val_acc: 0.2552\n",
      "Epoch 256/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1778 - acc: 0.2541 - val_loss: 2.1777 - val_acc: 0.2560\n",
      "Epoch 257/300\n",
      "2000/2000 [==============================] - 71s 35ms/step - loss: 2.1754 - acc: 0.2541 - val_loss: 2.1784 - val_acc: 0.2532\n",
      "Epoch 258/300\n",
      "2000/2000 [==============================] - 71s 35ms/step - loss: 2.1741 - acc: 0.2541 - val_loss: 2.1755 - val_acc: 0.2568\n",
      "Epoch 259/300\n",
      "2000/2000 [==============================] - 71s 35ms/step - loss: 2.1735 - acc: 0.2542 - val_loss: 2.1942 - val_acc: 0.1840\n",
      "Epoch 260/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1672 - acc: 0.2580 - val_loss: 2.1982 - val_acc: 0.1826\n",
      "Epoch 261/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1628 - acc: 0.2579 - val_loss: 2.1631 - val_acc: 0.2616\n",
      "Epoch 262/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1710 - acc: 0.2564 - val_loss: 2.1628 - val_acc: 0.2614\n",
      "Epoch 263/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1784 - acc: 0.2538 - val_loss: 2.1737 - val_acc: 0.2576\n",
      "Epoch 264/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1783 - acc: 0.2542 - val_loss: 2.1777 - val_acc: 0.2540\n",
      "Epoch 265/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1780 - acc: 0.2547 - val_loss: 2.1720 - val_acc: 0.2562\n",
      "Epoch 266/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1780 - acc: 0.2541 - val_loss: 2.1748 - val_acc: 0.2560\n",
      "Epoch 267/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1781 - acc: 0.2543 - val_loss: 2.1756 - val_acc: 0.2558\n",
      "Epoch 268/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1763 - acc: 0.2544 - val_loss: 2.1786 - val_acc: 0.2560\n",
      "Epoch 269/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1757 - acc: 0.2535 - val_loss: 2.1796 - val_acc: 0.2536\n",
      "Epoch 270/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1732 - acc: 0.2543 - val_loss: 2.1794 - val_acc: 0.2560\n",
      "Epoch 271/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1720 - acc: 0.2562 - val_loss: 2.2155 - val_acc: 0.1842\n",
      "Epoch 272/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1642 - acc: 0.2572 - val_loss: 2.1861 - val_acc: 0.1828\n",
      "Epoch 273/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1614 - acc: 0.2587 - val_loss: 2.1655 - val_acc: 0.2612\n",
      "Epoch 274/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1797 - acc: 0.2537 - val_loss: 2.1615 - val_acc: 0.2614\n",
      "Epoch 275/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1789 - acc: 0.2534 - val_loss: 2.1730 - val_acc: 0.2582\n",
      "Epoch 276/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1781 - acc: 0.2545 - val_loss: 2.1755 - val_acc: 0.2542\n",
      "Epoch 277/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1773 - acc: 0.2545 - val_loss: 2.1717 - val_acc: 0.2554\n",
      "Epoch 278/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1786 - acc: 0.2539 - val_loss: 2.1740 - val_acc: 0.2568\n",
      "Epoch 279/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1779 - acc: 0.2538 - val_loss: 2.1766 - val_acc: 0.2554\n",
      "Epoch 280/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1759 - acc: 0.2538 - val_loss: 2.1808 - val_acc: 0.2558\n",
      "Epoch 281/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1743 - acc: 0.2536 - val_loss: 2.1796 - val_acc: 0.2548\n",
      "Epoch 282/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1736 - acc: 0.2538 - val_loss: 2.1914 - val_acc: 0.2544\n",
      "Epoch 283/300\n",
      "2000/2000 [==============================] - 69s 35ms/step - loss: 2.1678 - acc: 0.2576 - val_loss: 2.1971 - val_acc: 0.1848\n",
      "Epoch 284/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1628 - acc: 0.2577 - val_loss: 2.1758 - val_acc: 0.2552\n",
      "Epoch 285/300\n",
      "2000/2000 [==============================] - 69s 34ms/step - loss: 2.1707 - acc: 0.2563 - val_loss: 2.1654 - val_acc: 0.2606\n",
      "Epoch 286/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1787 - acc: 0.2533 - val_loss: 2.1605 - val_acc: 0.2610\n",
      "Epoch 287/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1785 - acc: 0.2538 - val_loss: 2.1719 - val_acc: 0.2588\n",
      "Epoch 288/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1782 - acc: 0.2546 - val_loss: 2.1743 - val_acc: 0.2546\n",
      "Epoch 289/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1781 - acc: 0.2535 - val_loss: 2.1710 - val_acc: 0.2548\n",
      "Epoch 290/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1788 - acc: 0.2537 - val_loss: 2.1734 - val_acc: 0.2570\n",
      "Epoch 291/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1761 - acc: 0.2542 - val_loss: 2.1784 - val_acc: 0.2554\n",
      "Epoch 292/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1760 - acc: 0.2530 - val_loss: 2.1816 - val_acc: 0.2556\n",
      "Epoch 293/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1736 - acc: 0.2540 - val_loss: 2.1831 - val_acc: 0.2550\n",
      "Epoch 294/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1723 - acc: 0.2560 - val_loss: 2.2140 - val_acc: 0.1844\n",
      "Epoch 295/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1645 - acc: 0.2569 - val_loss: 2.1847 - val_acc: 0.1850\n",
      "Epoch 296/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1612 - acc: 0.2586 - val_loss: 2.1815 - val_acc: 0.2558\n",
      "Epoch 297/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1801 - acc: 0.2532 - val_loss: 2.1650 - val_acc: 0.2602\n",
      "Epoch 298/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1788 - acc: 0.2534 - val_loss: 2.1611 - val_acc: 0.2608\n",
      "Epoch 299/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1783 - acc: 0.2545 - val_loss: 2.1674 - val_acc: 0.2590\n",
      "Epoch 300/300\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1777 - acc: 0.2542 - val_loss: 2.1750 - val_acc: 0.2548\n"
     ]
    }
   ],
   "source": [
    "history = vggmodel.fit_generator(generator=tile_generator(landsat_datasets, label_dataset, 32, 32, train_px, batch_size), \n",
    "                    steps_per_epoch=len(train_px) // batch_size, epochs=300, verbose=1,\n",
    "                    validation_data=tile_generator(landsat_datasets, label_dataset, 32, 32, val_px, batch_size),\n",
    "                    validation_steps=len(val_px) // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:42<00:00, 10.60s/it]\n"
     ]
    }
   ],
   "source": [
    "test_px, val_px = gen_pixel_locations(landsat_datasets, train_count=500, val_count=0, tile_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 8s 375ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.177214562892914, 0.27999999821186067]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vggmodel.evaluate_generator(generator=tile_generator(landsat_datasets, label_dataset, 32, 32, test_px, batch_size), \n",
    "                        steps=len(test_px) // batch_size,\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model = keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/rasterio/plot.py:319: RuntimeWarning: overflow encountered in short_scalars\n",
      "  band_normed = (band - imin) / (imax - imin)\n"
     ]
    }
   ],
   "source": [
    "inception_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "inception_model.fit_generator(generator=tile_generator(landsat_datasets, label_dataset, 64, 64, train_px, batch_size), \n",
    "                    steps_per_epoch=len(train_px) // batch_size, epochs=50, verbose=1,\n",
    "                    validation_data=tile_generator(landsat_datasets, label_dataset, 64, 64, val_px, batch_size),\n",
    "                    validation_steps=len(val_px) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
